# Chat with PDFs - Developer Documentation

This documentation explains how the application enables users to chat with PDF documents. It covers the key components of the codebase and provides insights into how the system works, allowing developers to understand, modify, or extend the functionality without breaking the code.

## Overview

The application allows users to upload PDF books and interact with them through a chat interface. The system leverages the following technologies:

- **Flask**: A lightweight web framework used to build the web application.
- **Chroma**: A vector store for storing document embeddings.
- **LangChain**: A library for building language model applications.
- **OpenAI API**: Provides the language model for generating responses.
- **SentenceTransformers**: Used for embedding texts.

## Architecture

The application flow involves the following key components:

1. **Routes**:
   - `main_routes.py`: Handles the main application routes, including setting up the chat session.
   - `chat_routes.py`: Handles the chat endpoint where user messages are processed.

2. **Services**:
   - `chroma_utils.py`: Manages the interaction with the Chroma vector store and sets up the QA retrieval chain.
   - `openai_utils.py`: Handles communication with the OpenAI API to generate responses.

3. **Templates**:
   - `chat.html`: The frontend interface where users interact with the chat.

## Detailed Explanation

### 1. `main_routes.py`

This module handles the primary routes of the application, including setting up the session for the chat feature.

#### **Route: `/chat/<int:book_id>`**

- **Purpose**: Displays the chat interface for a specific book and prepares the necessary context for the chat session.
- **Key Actions**:
  - **Retrieve Book Information**: Fetches the book from the database using the provided `book_id`.
  - **Prepare File Paths**:
    - **`file_path`**: The path to the book's PDF file as stored in the database (e.g., `'app/static/storage/books/Cybersecurity/Book.pdf'`).
    - **`relative_path`**: Adjusted file path for serving the PDF in the browser by removing the `'app/static/'` prefix.
    - **`pdf_url`**: URL used in the frontend to display the PDF using Flask's `url_for` function.
  - **Determine `book_directory`**:
    - **Purpose**: The directory containing the book's PDF and the corresponding `chroma_db` directory.
    - **Process**:
      1. **Adjust `file_path`**: Removes `'app/static/'` to get a path relative to the `static` directory.
      2. **Construct Absolute Path**: Joins the application's static directory (`STORAGE_DIR` from the config) with the adjusted `file_path`.
      3. **Get Directory**: Uses `os.path.dirname` to retrieve the directory containing the book and `chroma_db`.
  - **Set Session Variable**:
    - Stores `book_directory` in the session to be accessed later in the chat processing.
  - **Debug Logs**:
    - Prints various paths to verify correctness during development.
- **Template Rendering**:
  - Renders `chat.html` with `book_title` and `pdf_url` for display in the frontend.

#### **Key Variables**:

- `book_title`: Title of the book for display purposes.
- `pdf_url`: URL for the PDF to be embedded in the chat interface.
- `book_directory`: Absolute path to the directory containing the book and `chroma_db`.

#### **Important Notes**:

- **Session Management**:
  - `book_directory` is stored in the session to maintain context across requests.
- **File Paths**:
  - Accurate path manipulation is crucial to ensure that the application can locate the `chroma_db` directory for each book.
- **Configuration**:
  - `STORAGE_DIR` in the application's configuration should point to `'app/static'`.

### 2. `chat_routes.py`

This module handles the chat functionality, receiving user messages and generating responses.

#### **Route: `/chat`**

- **Method**: `POST`
- **Purpose**: Processes the user's chat message and returns a response generated by the language model.
- **Key Actions**:
  - **Retrieve User Message**: Extracts the message from the POST request's JSON payload.
  - **Access `book_directory`**:
    - Retrieves `book_directory` from the session to identify which book's context to use.
  - **Construct Paths and Identifiers**:
    - **`chroma_db_path`**: Path to the Chroma database directory (`book_directory/chroma_db`).
    - **`collection_name`**: Name of the Chroma collection, typically the name of the book's directory.
  - **Initialize QA Chain**:
    - Calls `get_qa_chain` from `chroma_utils.py` with `chroma_db_path` and `collection_name`.
  - **Generate Response**:
    - Calls `get_chat_response` from `openai_utils.py` with the user's message and the QA chain.
  - **Return Response**:
    - Sends the generated response back to the frontend as JSON.
  - **Debug Logs**:
    - Prints diagnostic information to assist in troubleshooting.

#### **Error Handling**:

- **Missing Message**: Returns a 400 error if no message is provided.
- **Missing `book_directory`**: Returns a 400 error if `book_directory` is not found in the session.

#### **Important Notes**:

- **Session Dependency**:
  - Relies on `book_directory` being correctly set in the session by `main_routes.py`.
- **Debugging**:
  - Debug statements help ensure that API keys and paths are correctly configured.

### 3. `chroma_utils.py`

This module manages the retrieval QA chain, which combines the Chroma vector store and the language model to generate context-aware responses.

#### **Function: `get_qa_chain(chroma_db_path: str, collection_name: str)`**

- **Purpose**: Initializes and returns a `RetrievalQA` chain configured to use the specified Chroma collection and embeddings.
- **Key Actions**:
  - **Initialize Embeddings**:
    - Uses `SentenceTransformerEmbeddings` with the `'sentence-transformers/all-MiniLM-L6-v2'` model.
  - **Set Up Vector Store**:
    - Creates a `Chroma` vector store instance with the given `collection_name` and `chroma_db_path`.
  - **Test Retrieval** (Debugging):
    - Performs a test query (e.g., `"What is the book about?"`) to verify that documents are being retrieved.
    - Prints the retrieved documents for inspection.
  - **Define Prompt Template**:
    - Customizes the prompt used by the language model to generate responses, including instructions and placeholders for context and question.
  - **Configure Language Model**:
    - Retrieves the OpenAI API key from the application configuration.
  - **Create RetrievalQA Chain**:
    - Combines the vector store retriever and the language model into a `RetrievalQA` chain.
  - **Return QA Chain**.

#### **Classes**:

- **`SentenceTransformerEmbeddings`**:
  - A custom embeddings class that wraps the `SentenceTransformer` model.
  - Implements methods to embed queries and documents.

#### **Important Notes**:

- **Embeddings Consistency**:
  - Ensure that the same embeddings model is used during both indexing and retrieval.
- **API Key Usage**:
  - The OpenAI API key must be correctly set in the application's configuration.
- **Debugging Retrieval**:
  - The test retrieval helps verify that the vector store is properly populated and accessible.

### 4. `openai_utils.py`

This module handles the interaction with the OpenAI API to generate responses using the language model.

#### **Function: `get_chat_response(query: str, qa_chain: RetrievalQA) -> str`**

- **Purpose**: Invokes the QA chain with the user's query to generate a response.
- **Key Actions**:
  - **Retrieve API Key**:
    - Gets the OpenAI API key from the application configuration.
  - **Invoke QA Chain**:
    - Calls the `invoke` method on the `RetrievalQA` chain with the query.
  - **Extract Result**:
    - Retrieves the generated response from the chain's output.
  - **Return Response**.
  - **Debug Logs**:
    - Prints the API key (masked) for verification during development.

#### **Important Notes**:

- **Error Handling**:
  - Ensure that the API key is valid and not `None`.
- **Security**:
  - Mask API keys when logging to prevent exposing sensitive information.
- **Chain Invocation**:
  - The `invoke` method returns a dictionary; extract the `"result"` key to get the actual response.

## Workflow Summary

1. **User Accesses Chat Interface**:
   - Navigates to `/chat/<book_id>`.
   - The application sets up the necessary session variables and renders the chat interface with the embedded PDF.

2. **User Sends a Message**:
   - Enters a query in the chat interface.
   - The frontend sends a POST request to `/chat` with the message.

3. **Message Processing**:
   - `chat_routes.py` receives the message.
   - Retrieves `book_directory` from the session.
   - Constructs `chroma_db_path` and `collection_name`.
   - Initializes the QA chain via `chroma_utils.py`.

4. **Response Generation**:
   - The QA chain retrieves relevant documents from the Chroma vector store based on the user's query.
   - The context and query are passed to the OpenAI language model.
   - The model generates a response considering the provided context.

5. **Response Delivery**:
   - The generated response is returned to the frontend.
   - The user sees the assistant's reply in the chat interface.

## Modifying the Code

### Adding a New Feature

- **Update Embeddings Model**:
  - If you wish to change the embeddings model, update the `SentenceTransformerEmbeddings` class in `chroma_utils.py`.
  - Ensure that the same model is used during both indexing (when creating `chroma_db`) and retrieval.

- **Customize the Prompt**:
  - Modify the `template` string in `chroma_utils.py` to change how the assistant responds.
  - Adjust instructions, formatting, or add additional placeholders as needed.

### Changing the OpenAI Model

- **Update Model Parameters**:
  - In `chroma_utils.py`, when creating the `OpenAI` instance, you can specify different models or parameters.
  - Example:
    ```python
    llm=OpenAI(api_key=openai_api_key, model_name='gpt-3.5-turbo', temperature=0.7),
    ```

- **Handle API Key**:
  - Ensure that the API key supports the chosen model and that you have sufficient quota.

### Error Handling Enhancements

- **Improve User Feedback**:
  - Modify `chat_routes.py` to catch exceptions during the response generation and return user-friendly error messages.
  - Implement try-except blocks around critical operations.

### Debugging and Logging

- **Adjust Debug Statements**:
  - Use logging libraries like Python's built-in `logging` module instead of `print` statements for better control over log levels and outputs.
  - Configure loggers to write to files or external systems for production environments.

### Frontend Modifications

- **Update `chat.html`**:
  - Customize the chat interface to improve user experience.
  - Add features like message history, formatting options, or support for multiple PDFs.

### Security Considerations

- **Protect API Keys**:
  - Never expose API keys in the code or logs.
  - Use environment variables or secure configuration management.

- **Validate User Inputs**:
  - Ensure that all user inputs are properly sanitized to prevent injection attacks.

## Conclusion

This documentation provides an overview of how the application enables users to chat with PDFs by integrating Flask routes, Chroma vector storage, LangChain's retrieval QA chain, and the OpenAI API. By understanding the flow and components, developers can confidently modify and extend the application's functionality.

## Additional Resources

- **Flask Documentation**: [https://flask.palletsprojects.com/](https://flask.palletsprojects.com/)
- **Chroma**: [https://docs.trychroma.com/](https://docs.trychroma.com/)
- **LangChain**: [https://langchain.readthedocs.io/](https://langchain.readthedocs.io/)
- **OpenAI API**: [https://beta.openai.com/docs/](https://beta.openai.com/docs/)
- **SentenceTransformers**: [https://www.sbert.net/](https://www.sbert.net/)
